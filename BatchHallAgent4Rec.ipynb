{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import faiss\n",
    "from typing import Dict, List, Optional, Tuple, Set\n",
    "from datetime import datetime, timedelta\n",
    "import math\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.docstore import InMemoryDocstore\n",
    "from langchain.retrievers import TimeWeightedVectorStoreRetriever\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings\n",
    "from langchain_experimental.generative_agents import (\n",
    "    GenerativeAgent,\n",
    "    GenerativeAgentMemory,\n",
    ")\n",
    "\n",
    "\n",
    "# Base memory structure (as provided in the notebook)\n",
    "class MemoryItem(BaseModel):\n",
    "    content: str\n",
    "    created_at: datetime\n",
    "    importance: Optional[float] = 0.0\n",
    "\n",
    "\n",
    "class BaseCache(BaseModel):\n",
    "    memories: Dict[str, MemoryItem] = Field(default_factory=dict)\n",
    "\n",
    "\n",
    "# Initialize the LLM with Google Generative AI\n",
    "LLM = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-2.0-flash-001\",\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    timeout=None,\n",
    "    max_retries=2,\n",
    ")\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
    "\n",
    "\n",
    "# Helper functions for agent creation (from the notebook)\n",
    "def relevance_score_fn(score: float) -> float:\n",
    "    \"\"\"Return a similarity score on a scale [0, 1].\"\"\"\n",
    "    # Convert euclidean norm of normalized embeddings to similarity function\n",
    "    return 1.0 - score / math.sqrt(2)\n",
    "\n",
    "\n",
    "def create_new_memory_retriever():\n",
    "    \"\"\"Create a new vector store retriever unique to the agent.\"\"\"\n",
    "    # Initialize the vectorstore as empty\n",
    "    embedding_size = 768  # fixed to match with GoogleEmbedding\n",
    "    index = faiss.IndexFlatL2(embedding_size)\n",
    "    vectorstore = FAISS(\n",
    "        embeddings_model.embed_query,\n",
    "        index,\n",
    "        InMemoryDocstore({}),\n",
    "        {},\n",
    "        relevance_score_fn=relevance_score_fn,\n",
    "    )\n",
    "    return TimeWeightedVectorStoreRetriever(\n",
    "        vectorstore=vectorstore, other_score_keys=[\"importance\"], k=15\n",
    "    )\n",
    "\n",
    "\n",
    "def interview_agent(agent: GenerativeAgent, message: str) -> str:\n",
    "    \"\"\"Help interact with the agent.\"\"\"\n",
    "    new_message = f\"{agent.name} says {message}\"\n",
    "    return agent.generate_dialogue_response(new_message)[1]\n",
    "\n",
    "\n",
    "# =========== HallAgent4Rec Implementation Based on the Paper ===========\n",
    "\n",
    "class HallAgent4Rec:\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_clusters: int = 10,\n",
    "        latent_dim: int = 20,\n",
    "        lambda_u: float = 0.1,\n",
    "        lambda_v: float = 0.1,\n",
    "        lambda_h: float = 1.0,\n",
    "        learning_rate: float = 0.01,\n",
    "        decay_rate: float = 0.0001,\n",
    "        max_iterations: int = 100,\n",
    "        similarity_threshold: float = 0.5,\n",
    "        relevance_threshold: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize the HallAgent4Rec system.\n",
    "        \n",
    "        Args:\n",
    "            num_clusters: Number of clusters for item grouping\n",
    "            latent_dim: Dimensionality of user and item embeddings\n",
    "            lambda_u: User regularization coefficient\n",
    "            lambda_v: Item regularization coefficient\n",
    "            lambda_h: Hallucination penalty coefficient\n",
    "            learning_rate: Initial learning rate for optimization\n",
    "            decay_rate: Learning rate decay parameter\n",
    "            max_iterations: Maximum number of iterations for matrix factorization\n",
    "            similarity_threshold: Threshold for item similarity in retrieval\n",
    "            relevance_threshold: Threshold for item relevance in knowledge base\n",
    "        \"\"\"\n",
    "        self.num_clusters = num_clusters\n",
    "        self.latent_dim = latent_dim\n",
    "        self.lambda_u = lambda_u\n",
    "        self.lambda_v = lambda_v\n",
    "        self.lambda_h = lambda_h\n",
    "        self.learning_rate = learning_rate\n",
    "        self.decay_rate = decay_rate\n",
    "        self.max_iterations = max_iterations\n",
    "        self.similarity_threshold = similarity_threshold\n",
    "        self.relevance_threshold = relevance_threshold\n",
    "        \n",
    "        # Initialize components\n",
    "        self.agents = {}  # Dictionary to store generative agents\n",
    "        self.clusters = None  # Will store the cluster model\n",
    "        self.cluster_assignments = None  # Will store item cluster assignments\n",
    "        self.user_embeddings = None  # Will store user embeddings from PMF\n",
    "        self.item_embeddings = None  # Will store item embeddings from PMF\n",
    "        self.user_cluster_matrix = None  # Will store user-cluster interaction matrix\n",
    "        self.item_features = None  # Will store item features for clustering\n",
    "        self.item_database = None  # Will store the item database\n",
    "        self.hallucination_scores = None  # Will store hallucination likelihood scores\n",
    "        self.items_by_cluster = {}  # Will store items grouped by cluster\n",
    "\n",
    "    def load_data(self, user_data: pd.DataFrame, item_data: pd.DataFrame, interactions: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Load user, item, and interaction data.\n",
    "        \n",
    "        Args:\n",
    "            user_data: DataFrame with user information (id, features, etc.)\n",
    "            item_data: DataFrame with item information (id, features, etc.)\n",
    "            interactions: DataFrame with user-item interactions\n",
    "        \"\"\"\n",
    "        self.user_data = user_data\n",
    "        self.item_data = item_data\n",
    "        self.interactions = interactions\n",
    "        \n",
    "        # Create a set of all users and items\n",
    "        self.all_users = set(user_data['user_id'].unique())\n",
    "        self.all_items = set(item_data['item_id'].unique())\n",
    "        \n",
    "        # Extract item features for clustering\n",
    "        feature_columns = [col for col in item_data.columns if col != 'item_id']\n",
    "        self.item_features = item_data[feature_columns].values\n",
    "        self.item_id_map = {id: idx for idx, id in enumerate(item_data['item_id'])}\n",
    "        self.idx_to_item_id = {idx: id for id, idx in self.item_id_map.items()}\n",
    "        \n",
    "        # Create user-item interaction matrix\n",
    "        self.create_interaction_matrix()\n",
    "        print(f\"Loaded data: {len(self.all_users)} users, {len(self.all_items)} items, {len(interactions)} interactions\")\n",
    "\n",
    "    def create_interaction_matrix(self):\n",
    "        \"\"\"Create the user-item interaction matrix from interaction data.\"\"\"\n",
    "        # Create user and item ID mappings\n",
    "        self.user_id_map = {id: idx for idx, id in enumerate(self.user_data['user_id'])}\n",
    "        self.idx_to_user_id = {idx: id for id, idx in self.user_id_map.items()}\n",
    "        \n",
    "        # Initialize interaction matrix\n",
    "        num_users = len(self.user_id_map)\n",
    "        num_items = len(self.item_id_map)\n",
    "        self.interaction_matrix = np.zeros((num_users, num_items))\n",
    "        \n",
    "        # Fill the interaction matrix\n",
    "        for _, row in self.interactions.iterrows():\n",
    "            user_idx = self.user_id_map.get(row['user_id'])\n",
    "            item_idx = self.item_id_map.get(row['item_id'])\n",
    "            if user_idx is not None and item_idx is not None:\n",
    "                # For implicit feedback, use 1 to indicate interaction\n",
    "                self.interaction_matrix[user_idx, item_idx] = 1\n",
    "        \n",
    "        print(f\"Created interaction matrix of shape {self.interaction_matrix.shape}\")\n",
    "\n",
    "    def cluster_items(self):\n",
    "        \"\"\"Cluster items based on their features.\"\"\"\n",
    "        print(f\"Clustering {len(self.item_features)} items into {self.num_clusters} clusters...\")\n",
    "        \n",
    "        # Apply k-means clustering\n",
    "        self.clusters = KMeans(n_clusters=self.num_clusters, random_state=42)\n",
    "        self.cluster_assignments = self.clusters.fit_predict(self.item_features)\n",
    "        \n",
    "        # Group items by cluster\n",
    "        self.items_by_cluster = {}\n",
    "        for idx, cluster_id in enumerate(self.cluster_assignments):\n",
    "            if cluster_id not in self.items_by_cluster:\n",
    "                self.items_by_cluster[cluster_id] = []\n",
    "            self.items_by_cluster[cluster_id].append(idx)\n",
    "        \n",
    "        # Create user-cluster interaction matrix\n",
    "        self.create_user_cluster_matrix()\n",
    "        \n",
    "        print(f\"Item clustering complete. Cluster distribution:\")\n",
    "        for cluster_id, items in self.items_by_cluster.items():\n",
    "            print(f\"Cluster {cluster_id}: {len(items)} items\")\n",
    "\n",
    "    def create_user_cluster_matrix(self):\n",
    "        \"\"\"\n",
    "        Create user-cluster interaction matrix by aggregating \n",
    "        user-item interactions within each cluster.\n",
    "        \"\"\"\n",
    "        num_users = self.interaction_matrix.shape[0]\n",
    "        self.user_cluster_matrix = np.zeros((num_users, self.num_clusters))\n",
    "        \n",
    "        # For each user-cluster pair, aggregate interactions\n",
    "        for user_idx in range(num_users):\n",
    "            for cluster_id in range(self.num_clusters):\n",
    "                # Get items in this cluster\n",
    "                cluster_items = self.items_by_cluster[cluster_id]\n",
    "                # Sum interactions with items in this cluster\n",
    "                cluster_interactions = sum(self.interaction_matrix[user_idx, item_idx] for item_idx in cluster_items)\n",
    "                # Normalize by cluster size to get average interaction\n",
    "                self.user_cluster_matrix[user_idx, cluster_id] = cluster_interactions / max(1, len(cluster_items))\n",
    "        \n",
    "        print(f\"Created user-cluster matrix of shape {self.user_cluster_matrix.shape}\")\n",
    "\n",
    "    def matrix_factorization(self):\n",
    "        \"\"\"\n",
    "        Perform hallucination-aware matrix factorization to learn\n",
    "        user and item latent factors.\n",
    "        \"\"\"\n",
    "        print(\"Starting hallucination-aware matrix factorization...\")\n",
    "        \n",
    "        # Initialize user and item embeddings randomly\n",
    "        num_users = self.interaction_matrix.shape[0]\n",
    "        num_items = self.interaction_matrix.shape[1]\n",
    "        \n",
    "        # Initialize embeddings\n",
    "        self.user_embeddings = np.random.normal(0, 0.1, (num_users, self.latent_dim))\n",
    "        self.item_embeddings = np.random.normal(0, 0.1, (num_items, self.latent_dim))\n",
    "        \n",
    "        # Initialize hallucination likelihood scores\n",
    "        # Initially, all items have equal hallucination likelihood\n",
    "        self.hallucination_scores = np.ones((num_users, num_items)) * 0.5\n",
    "        \n",
    "        # Get user-item pairs with observed interactions\n",
    "        user_indices, item_indices = np.where(self.interaction_matrix > 0)\n",
    "        \n",
    "        # Prepare for optimization\n",
    "        learning_rate = self.learning_rate\n",
    "        \n",
    "        # Implement mini-batch SGD as described in Algorithm 1 of the paper\n",
    "        for iteration in range(self.max_iterations):\n",
    "            # Shuffle the observed interactions\n",
    "            indices = np.arange(len(user_indices))\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            # Mini-batch size\n",
    "            batch_size = min(1024, len(indices))\n",
    "            \n",
    "            # Process mini-batches\n",
    "            for start in range(0, len(indices), batch_size):\n",
    "                end = min(start + batch_size, len(indices))\n",
    "                batch_indices = indices[start:end]\n",
    "                \n",
    "                # Get user and item indices for this batch\n",
    "                batch_user_indices = user_indices[batch_indices]\n",
    "                batch_item_indices = item_indices[batch_indices]\n",
    "                \n",
    "                # Update user embeddings\n",
    "                for i, user_idx in enumerate(batch_user_indices):\n",
    "                    item_idx = batch_item_indices[i]\n",
    "                    \n",
    "                    # Get observed interaction\n",
    "                    rating = self.interaction_matrix[user_idx, item_idx]\n",
    "                    \n",
    "                    # Compute prediction\n",
    "                    prediction = np.dot(self.user_embeddings[user_idx], self.item_embeddings[item_idx])\n",
    "                    \n",
    "                    # Compute error\n",
    "                    error = rating - prediction\n",
    "                    \n",
    "                    # Hallucination penalty term (from equation 22 in the paper)\n",
    "                    h_penalty = self.lambda_h * self.hallucination_scores[user_idx, item_idx] * prediction\n",
    "                    \n",
    "                    # Compute gradients\n",
    "                    user_grad = -error * self.item_embeddings[item_idx] + self.lambda_u * self.user_embeddings[user_idx] + h_penalty * self.item_embeddings[item_idx]\n",
    "                    item_grad = -error * self.user_embeddings[user_idx] + self.lambda_v * self.item_embeddings[item_idx] + h_penalty * self.user_embeddings[user_idx]\n",
    "                    \n",
    "                    # Update embeddings\n",
    "                    self.user_embeddings[user_idx] -= learning_rate * user_grad\n",
    "                    self.item_embeddings[item_idx] -= learning_rate * item_grad\n",
    "            \n",
    "            # Decay learning rate (equation 27 in the paper)\n",
    "            learning_rate = self.learning_rate / (1 + self.decay_rate * iteration)\n",
    "            \n",
    "            # Print progress every 10 iterations\n",
    "            if (iteration + 1) % 10 == 0:\n",
    "                # Compute training error\n",
    "                error = self.compute_training_error()\n",
    "                print(f\"Iteration {iteration + 1}/{self.max_iterations}, Error: {error:.4f}, Learning Rate: {learning_rate:.6f}\")\n",
    "        \n",
    "        print(\"Matrix factorization complete\")\n",
    "\n",
    "    def compute_training_error(self):\n",
    "        \"\"\"Compute the training error of the current matrix factorization model.\"\"\"\n",
    "        # Compute predictions for all observed interactions\n",
    "        user_indices, item_indices = np.where(self.interaction_matrix > 0)\n",
    "        ratings = self.interaction_matrix[user_indices, item_indices]\n",
    "        \n",
    "        # Compute predictions\n",
    "        predictions = np.sum(self.user_embeddings[user_indices] * self.item_embeddings[item_indices], axis=1)\n",
    "        \n",
    "        # Compute mean squared error\n",
    "        mse = np.mean((ratings - predictions) ** 2)\n",
    "        return mse\n",
    "\n",
    "    def initialize_agents(self):\n",
    "        \"\"\"Initialize generative agents for each user.\"\"\"\n",
    "        print(\"Initializing generative agents...\")\n",
    "        \n",
    "        for user_id, user_idx in self.user_id_map.items():\n",
    "            # Get user data\n",
    "            user_row = self.user_data[self.user_data['user_id'] == user_id].iloc[0]\n",
    "            \n",
    "            # Extract user traits from user data\n",
    "            traits = {}\n",
    "            for col in self.user_data.columns:\n",
    "                if col != 'user_id':\n",
    "                    traits[col] = user_row[col]\n",
    "            \n",
    "            # Format traits as a string\n",
    "            traits_str = \", \".join([f\"{k}: {v}\" for k, v in traits.items()])\n",
    "            \n",
    "            # Create agent memory\n",
    "            memory = GenerativeAgentMemory(\n",
    "                llm=LLM,\n",
    "                memory_retriever=create_new_memory_retriever(),\n",
    "                verbose=False,\n",
    "                reflection_threshold=30,\n",
    "            )\n",
    "            \n",
    "            # Create generative agent\n",
    "            agent = GenerativeAgent(\n",
    "                name=f\"User_{user_id}\",\n",
    "                age=traits.get('age', 30),  # Default to 30 if age not provided\n",
    "                traits=traits_str,\n",
    "                status=\"looking for recommendations\",\n",
    "                memory_retriever=create_new_memory_retriever(),\n",
    "                llm=LLM,\n",
    "                memory=memory,\n",
    "            )\n",
    "            \n",
    "            # Store agent\n",
    "            self.agents[user_id] = agent\n",
    "            \n",
    "            # Add user interactions as memories to the agent\n",
    "            user_interactions = self.interactions[self.interactions['user_id'] == user_id]\n",
    "            for _, interaction in user_interactions.iterrows():\n",
    "                item_id = interaction['item_id']\n",
    "                if 'timestamp' in interaction:\n",
    "                    timestamp = interaction['timestamp']\n",
    "                    created_at = datetime.fromtimestamp(timestamp) if isinstance(timestamp, (int, float)) else datetime.now()\n",
    "                else:\n",
    "                    created_at = datetime.now()\n",
    "                \n",
    "                # Get item details\n",
    "                item_row = self.item_data[self.item_data['item_id'] == item_id]\n",
    "                if not item_row.empty:\n",
    "                    item_name = item_row.iloc[0].get('name', f\"Item_{item_id}\")\n",
    "                    memory_content = f\"I interacted with {item_name} (ID: {item_id})\"\n",
    "                    \n",
    "                    # Add to agent memory\n",
    "                    agent.memory.add_memory(memory_content, created_at=created_at)\n",
    "        \n",
    "        print(f\"Initialized {len(self.agents)} generative agents\")\n",
    "\n",
    "    def construct_rag_query(self, user_id):\n",
    "        \"\"\"\n",
    "        Construct a retrieval query integrating user traits and memory.\n",
    "        This implements equation 12 from the paper.\n",
    "        \"\"\"\n",
    "        # Get agent for the user\n",
    "        agent = self.agents[user_id]\n",
    "        \n",
    "        # Get user traits\n",
    "        user_traits = agent.traits\n",
    "        \n",
    "        # Get relevant memories\n",
    "        relevant_memories = agent.memory.memory_retriever.get_relevant_documents(\"What do I like?\")\n",
    "        memory_contents = \" \".join([mem.page_content for mem in relevant_memories])\n",
    "        \n",
    "        # Construct the query\n",
    "        query = f\"User traits: {user_traits}. User memories: {memory_contents}\"\n",
    "        \n",
    "        # Encode the query\n",
    "        query_embedding = embeddings_model.embed_query(query)\n",
    "        \n",
    "        return query, query_embedding\n",
    "\n",
    "    def construct_knowledge_base(self, user_id):\n",
    "        \"\"\"\n",
    "        Construct a knowledge base of relevant items for the user.\n",
    "        This implements equation 14 from the paper.\n",
    "        \"\"\"\n",
    "        # Get user index\n",
    "        user_idx = self.user_id_map[user_id]\n",
    "        \n",
    "        # Predict user's cluster preferences\n",
    "        cluster_scores = np.dot(self.user_embeddings[user_idx], self.item_embeddings.T)\n",
    "        \n",
    "        # Get the top cluster\n",
    "        top_cluster = np.argmax(np.mean([cluster_scores[user_idx] for user_idx in self.items_by_cluster[cluster_id]]) \n",
    "                                for cluster_id in range(self.num_clusters))\n",
    "        \n",
    "        # Get items in the top cluster\n",
    "        cluster_items = self.items_by_cluster[top_cluster]\n",
    "        \n",
    "        # Get item scores\n",
    "        item_scores = np.dot(self.user_embeddings[user_idx], self.item_embeddings[cluster_items].T)\n",
    "        \n",
    "        # Filter items by relevance threshold\n",
    "        relevant_items = [cluster_items[i] for i, score in enumerate(item_scores) if score >= self.relevance_threshold]\n",
    "        \n",
    "        # Create knowledge base with item details\n",
    "        knowledge_base = []\n",
    "        for item_idx in relevant_items:\n",
    "            item_id = self.idx_to_item_id[item_idx]\n",
    "            item_row = self.item_data[self.item_data['item_id'] == item_id]\n",
    "            if not item_row.empty:\n",
    "                item_info = {}\n",
    "                for col in item_row.columns:\n",
    "                    item_info[col] = item_row.iloc[0][col]\n",
    "                knowledge_base.append(item_info)\n",
    "        \n",
    "        return knowledge_base, top_cluster\n",
    "\n",
    "    def retrieve_items(self, user_id, query_embedding, knowledge_base):\n",
    "        \"\"\"\n",
    "        Retrieve items from the knowledge base based on similarity to query.\n",
    "        This implements equation 15 from the paper.\n",
    "        \"\"\"\n",
    "        # Encode each item in the knowledge base\n",
    "        item_embeddings = []\n",
    "        for item in knowledge_base:\n",
    "            # Convert item to string representation\n",
    "            item_str = \", \".join([f\"{k}: {v}\" for k, v in item.items() if k != 'item_id'])\n",
    "            item_embedding = embeddings_model.embed_query(item_str)\n",
    "            item_embeddings.append((item, item_embedding))\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = []\n",
    "        for item, item_embedding in item_embeddings:\n",
    "            sim = cosine_similarity([query_embedding], [item_embedding])[0][0]\n",
    "            similarities.append((item, sim))\n",
    "        \n",
    "        # Sort by similarity and filter by threshold\n",
    "        retrieved_items = [item for item, sim in sorted(similarities, key=lambda x: x[1], reverse=True) \n",
    "                          if sim >= self.similarity_threshold]\n",
    "        \n",
    "        return retrieved_items\n",
    "\n",
    "    def generate_recommendations(self, user_id, num_recommendations=5):\n",
    "        \"\"\"\n",
    "        Generate recommendations for a user using the HallAgent4Rec methodology.\n",
    "        This implements the full recommendation pipeline from the paper.\n",
    "        \"\"\"\n",
    "        print(f\"Generating recommendations for user {user_id}...\")\n",
    "        \n",
    "        # Step 1: Construct RAG query (eq. 12-13)\n",
    "        query, query_embedding = self.construct_rag_query(user_id)\n",
    "        \n",
    "        # Step 2: Construct knowledge base (eq. 14)\n",
    "        knowledge_base, top_cluster = self.construct_knowledge_base(user_id)\n",
    "        \n",
    "        # Step 3: Retrieve relevant items (eq. 15)\n",
    "        retrieved_items = self.retrieve_items(user_id, query_embedding, knowledge_base)\n",
    "        \n",
    "        # If no items retrieved, return empty list\n",
    "        if not retrieved_items:\n",
    "            print(\"No relevant items retrieved. Cannot generate recommendations.\")\n",
    "            return []\n",
    "        \n",
    "        # Step 4: Generate recommendations using LLM\n",
    "        # Format retrieved items for prompt\n",
    "        item_descriptions = \"\\n\".join([f\"- {item['name']}\" for item in retrieved_items[:10]])\n",
    "        \n",
    "        # Get agent\n",
    "        agent = self.agents[user_id]\n",
    "        \n",
    "        # Create prompt for LLM\n",
    "        prompt = f\"\"\"\n",
    "        You are a recommendation system for a user with the following traits:\n",
    "        {agent.traits}\n",
    "        \n",
    "        Based on the user's profile and past behavior, you have retrieved the following relevant items:\n",
    "        {item_descriptions}\n",
    "        \n",
    "        Please recommend {num_recommendations} items from the list above that would be most relevant for this user.\n",
    "        For each recommendation, provide a brief explanation of why it matches the user's preferences.\n",
    "        \n",
    "        IMPORTANT: You must ONLY recommend items from the provided list. Do not suggest any items that are not in the list.\n",
    "        \n",
    "        Format your response as:\n",
    "        1. [Item Name]: [Explanation]\n",
    "        2. [Item Name]: [Explanation]\n",
    "        ...\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate recommendations\n",
    "        response = LLM.invoke(prompt)\n",
    "        recommendations_text = response.content\n",
    "        \n",
    "        # Step 5: Detect hallucinations\n",
    "        # Extract recommended items from response\n",
    "        recommended_items = []\n",
    "        lines = recommendations_text.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            if line.strip() and any(char.isdigit() for char in line[:5]):\n",
    "                # Extract item name from line (format: \"1. [Item Name]: [Explanation]\")\n",
    "                parts = line.split(':', 1)\n",
    "                if len(parts) > 0:\n",
    "                    item_name_part = parts[0].strip()\n",
    "                    # Extract text inside brackets if present\n",
    "                    item_name = item_name_part.split('.', 1)[1].strip() if '.' in item_name_part else item_name_part\n",
    "                    recommended_items.append(item_name)\n",
    "        \n",
    "        # Check for hallucinations (items not in retrieved set)\n",
    "        retrieved_item_names = [item['name'] for item in retrieved_items]\n",
    "        hallucinations = []\n",
    "        valid_recommendations = []\n",
    "        \n",
    "        for item_name in recommended_items:\n",
    "            is_hallucination = True\n",
    "            # Check if recommended item is in retrieved items (allowing for minor text differences)\n",
    "            for retrieved_name in retrieved_item_names:\n",
    "                if item_name.lower() in retrieved_name.lower() or retrieved_name.lower() in item_name.lower():\n",
    "                    is_hallucination = False\n",
    "                    # Find the actual item\n",
    "                    for item in retrieved_items:\n",
    "                        if item['name'].lower() in item_name.lower() or item_name.lower() in item['name'].lower():\n",
    "                            valid_recommendations.append(item)\n",
    "                            break\n",
    "                    break\n",
    "            \n",
    "            if is_hallucination:\n",
    "                hallucinations.append(item_name)\n",
    "        \n",
    "        # Report hallucinations\n",
    "        if hallucinations:\n",
    "            print(f\"Detected {len(hallucinations)} hallucinations: {hallucinations}\")\n",
    "            \n",
    "            # Update hallucination scores\n",
    "            user_idx = self.user_id_map[user_id]\n",
    "            for hallucination in hallucinations:\n",
    "                # For each hallucination, increase hallucination scores for similar items\n",
    "                for item_idx in range(self.item_embeddings.shape[0]):\n",
    "                    item_id = self.idx_to_item_id[item_idx]\n",
    "                    item_row = self.item_data[self.item_data['item_id'] == item_id]\n",
    "                    if not item_row.empty:\n",
    "                        item_name = item_row.iloc[0].get('name', f\"Item_{item_id}\")\n",
    "                        # If item name is similar to hallucination, increase score\n",
    "                        if hallucination.lower() in item_name.lower() or item_name.lower() in hallucination.lower():\n",
    "                            self.hallucination_scores[user_idx, item_idx] += 0.1\n",
    "        \n",
    "        # If not enough valid recommendations, fill with top predicted items\n",
    "        if len(valid_recommendations) < num_recommendations:\n",
    "            user_idx = self.user_id_map[user_id]\n",
    "            cluster_items = self.items_by_cluster[top_cluster]\n",
    "            \n",
    "            # Get predicted scores for items in the cluster\n",
    "            item_scores = [(idx, np.dot(self.user_embeddings[user_idx], self.item_embeddings[idx])) \n",
    "                           for idx in cluster_items]\n",
    "            \n",
    "            # Sort by score and remove items already recommended\n",
    "            recommended_ids = [item['item_id'] for item in valid_recommendations]\n",
    "            additional_items = []\n",
    "            \n",
    "            for item_idx, score in sorted(item_scores, key=lambda x: x[1], reverse=True):\n",
    "                item_id = self.idx_to_item_id[item_idx]\n",
    "                if item_id not in recommended_ids:\n",
    "                    item_row = self.item_data[self.item_data['item_id'] == item_id]\n",
    "                    if not item_row.empty:\n",
    "                        item_info = {}\n",
    "                        for col in item_row.columns:\n",
    "                            item_info[col] = item_row.iloc[0][col]\n",
    "                        additional_items.append(item_info)\n",
    "                        if len(valid_recommendations) + len(additional_items) >= num_recommendations:\n",
    "                            break\n",
    "            \n",
    "            valid_recommendations.extend(additional_items)\n",
    "        \n",
    "        # Limit to requested number\n",
    "        valid_recommendations = valid_recommendations[:num_recommendations]\n",
    "        \n",
    "        print(f\"Generated {len(valid_recommendations)} valid recommendations\")\n",
    "        return valid_recommendations\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the complete HallAgent4Rec model.\"\"\"\n",
    "        # Step 1: Cluster items\n",
    "        self.cluster_items()\n",
    "        \n",
    "        # Step 2: Initialize matrix factorization\n",
    "        self.matrix_factorization()\n",
    "        \n",
    "        # Step 3: Initialize generative agents\n",
    "        self.initialize_agents()\n",
    "        \n",
    "        print(\"HallAgent4Rec training complete!\")\n",
    "\n",
    "    def evaluate(self, test_interactions, metrics=['precision', 'recall', 'hallucination_rate']):\n",
    "        \"\"\"\n",
    "        Evaluate the recommendation system using test interactions.\n",
    "        \n",
    "        Args:\n",
    "            test_interactions: DataFrame with test user-item interactions\n",
    "            metrics: List of metrics to compute\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary of evaluation metrics\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        if 'precision' in metrics or 'recall' in metrics:\n",
    "            # Group test interactions by user\n",
    "            user_test_items = {}\n",
    "            for _, row in test_interactions.iterrows():\n",
    "                user_id = row['user_id']\n",
    "                item_id = row['item_id']\n",
    "                if user_id not in user_test_items:\n",
    "                    user_test_items[user_id] = set()\n",
    "                user_test_items[user_id].add(item_id)\n",
    "            \n",
    "            # Generate recommendations for each user\n",
    "            precision_sum = 0\n",
    "            recall_sum = 0\n",
    "            user_count = 0\n",
    "            \n",
    "            for user_id, test_items in user_test_items.items():\n",
    "                if user_id in self.user_id_map:\n",
    "                    # Generate recommendations\n",
    "                    recommendations = self.generate_recommendations(user_id, num_recommendations=10)\n",
    "                    if recommendations:\n",
    "                        recommended_items = [item['item_id'] for item in recommendations]\n",
    "                        \n",
    "                        # Compute precision and recall\n",
    "                        hit_count = len(set(recommended_items) & test_items)\n",
    "                        precision = hit_count / len(recommended_items) if recommended_items else 0\n",
    "                        recall = hit_count / len(test_items) if test_items else 0\n",
    "                        \n",
    "                        precision_sum += precision\n",
    "                        recall_sum += recall\n",
    "                        user_count += 1\n",
    "            \n",
    "            # Compute average precision and recall\n",
    "            if user_count > 0:\n",
    "                results['precision'] = precision_sum / user_count\n",
    "                results['recall'] = recall_sum / user_count\n",
    "        \n",
    "        if 'hallucination_rate' in metrics:\n",
    "            # Evaluate hallucination rate on a sample of users\n",
    "            hallucination_count = 0\n",
    "            total_recommendations = 0\n",
    "            \n",
    "            sample_users = np.random.choice(list(self.user_id_map.keys()), \n",
    "                                           size=min(50, len(self.user_id_map)), \n",
    "                                           replace=False)\n",
    "            \n",
    "            for user_id in sample_users:\n",
    "                # Construct query and knowledge base\n",
    "                query, query_embedding = self.construct_rag_query(user_id)\n",
    "                knowledge_base, _ = self.construct_knowledge_base(user_id)\n",
    "                retrieved_items = self.retrieve_items(user_id, query_embedding, knowledge_base)\n",
    "                \n",
    "                # Format retrieved items for prompt\n",
    "                item_descriptions = \"\\n\".join([f\"- {item['name']}\" for item in retrieved_items[:10]])\n",
    "                \n",
    "                # Get agent\n",
    "                agent = self.agents[user_id]\n",
    "                \n",
    "                # Create prompt for LLM\n",
    "                prompt = f\"\"\"\n",
    "                You are a recommendation system for a user with the following traits:\n",
    "                {agent.traits}\n",
    "                \n",
    "                Based on the user's profile and past behavior, you have retrieved the following relevant items:\n",
    "                {item_descriptions}\n",
    "                \n",
    "                Please recommend 5 items from the list above that would be most relevant for this user.\n",
    "                Provide a brief explanation for each recommendation.\n",
    "                \n",
    "                IMPORTANT: You must ONLY recommend items from the provided list. Do not suggest any items that are not in the list.\n",
    "                \n",
    "                Format your response as:\n",
    "                1. [Item Name]: [Explanation]\n",
    "                2. [Item Name]: [Explanation]\n",
    "                ...\n",
    "                \"\"\"\n",
    "                \n",
    "                # Generate recommendations\n",
    "                response = LLM.invoke(prompt)\n",
    "                recommendations_text = response.content\n",
    "                \n",
    "                # Extract recommended items from response\n",
    "                recommended_items = []\n",
    "                lines = recommendations_text.strip().split('\\n')\n",
    "                for line in lines:\n",
    "                    if line.strip() and any(char.isdigit() for char in line[:5]):\n",
    "                        parts = line.split(':', 1)\n",
    "                        if len(parts) > 0:\n",
    "                            item_name_part = parts[0].strip()\n",
    "                            item_name = item_name_part.split('.', 1)[1].strip() if '.' in item_name_part else item_name_part\n",
    "                            recommended_items.append(item_name)\n",
    "                \n",
    "                # Check for hallucinations\n",
    "                retrieved_item_names = [item['name'] for item in retrieved_items]\n",
    "                for item_name in recommended_items:\n",
    "                    total_recommendations += 1\n",
    "                    is_hallucination = True\n",
    "                    for retrieved_name in retrieved_item_names:\n",
    "                        if item_name.lower() in retrieved_name.lower() or retrieved_name.lower() in item_name.lower():\n",
    "                            is_hallucination = False\n",
    "                            break\n",
    "                    \n",
    "                    if is_hallucination:\n",
    "                        hallucination_count += 1\n",
    "            \n",
    "            # Compute hallucination rate\n",
    "            if total_recommendations > 0:\n",
    "                results['hallucination_rate'] = hallucination_count / total_recommendations\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# Example usage\n",
    "def example_usage():\n",
    "    # Create sample data\n",
    "    \n",
    "    # User data\n",
    "    user_data = pd.DataFrame({\n",
    "        'user_id': [1, 2, 3, 4, 5],\n",
    "        'age': [25, 30, 35, 40, 45],\n",
    "        'gender': ['M', 'F', 'M', 'F', 'M'],\n",
    "        'occupation': ['student', 'engineer', 'doctor', 'artist', 'teacher']\n",
    "    })\n",
    "    \n",
    "    # Item data\n",
    "    item_data = pd.DataFrame({\n",
    "        'item_id': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110],\n",
    "        'name': ['Item A', 'Item B', 'Item C', 'Item D', 'Item E', \n",
    "                 'Item F', 'Item G', 'Item H', 'Item I', 'Item J'],\n",
    "        'category': ['books', 'electronics', 'books', 'clothing', 'electronics',\n",
    "                    'books', 'clothing', 'electronics', 'books', 'clothing'],\n",
    "        'price': [10, 50, 15, 30, 100, 20, 25, 80, 12, 35],\n",
    "        'popularity': [0.8, 0.6, 0.9, 0.7, 0.5, 0.4, 0.7, 0.6, 0.3, 0.8]\n",
    "    })\n",
    "    \n",
    "    # Interaction data\n",
    "    interactions = pd.DataFrame({\n",
    "        'user_id': [1, 1, 1, 2, 2, 3, 3, 3, 4, 4, 5, 5, 5],\n",
    "        'item_id': [101, 103, 105, 102, 104, 103, 106, 109, 105, 107, 101, 108, 110],\n",
    "        'timestamp': [1615000000, 1615100000, 1615200000, 1615300000, 1615400000,\n",
    "                     1615500000, 1615600000, 1615700000, 1615800000, 1615900000,\n",
    "                     1616000000, 1616100000, 1616200000]\n",
    "    })\n",
    "    \n",
    "    # Test interactions\n",
    "    test_interactions = pd.DataFrame({\n",
    "        'user_id': [1, 2, 3, 4, 5],\n",
    "        'item_id': [106, 101, 102, 108, 103]\n",
    "    })\n",
    "    \n",
    "    # Initialize HallAgent4Rec\n",
    "    hall_agent = HallAgent4Rec(num_clusters=3, latent_dim=10)\n",
    "    \n",
    "    # Load data\n",
    "    hall_agent.load_data(user_data, item_data, interactions)\n",
    "    \n",
    "    # Train the system\n",
    "    hall_agent.train()\n",
    "    \n",
    "    # Generate recommendations for a user\n",
    "    recommendations = hall_agent.generate_recommendations(user_id=1, num_recommendations=3)\n",
    "    print(\"Recommendations for User 1:\")\n",
    "    for i, item in enumerate(recommendations):\n",
    "        print(f\"{i+1}. {item['name']} - {item['category']} (${item['price']})\")\n",
    "    \n",
    "    # Evaluate the system\n",
    "    eval_results = hall_agent.evaluate(test_interactions)\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    for metric, value in eval_results.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
